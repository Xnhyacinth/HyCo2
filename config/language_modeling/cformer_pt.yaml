## data
train_file: data/pretrain/train_data.jsonl
dev_file: data/pretrain/val_data.jsonl
max_seq_length: 2048
retrieval_context_length: 512
preprocessing_num_workers: 16
overwrite_cache: false
max_train_samples: 2000000

## model
model_name_or_path: models/mistralai__Mistral-7B-Instruct-v0.2
chat_format: mistral
# retriever_name_or_path: models/Salesforce__SFR-Embedding-Mistral
projector_type: cformer_2e-4
cformer_model_name_or_path: models/FacebookAI__roberta-base

## train
task_type: pretrain
workdir: ./
learning_rate: 2.0e-4
lr_scheduler_type: linear
warmup_ratio: 0.03
weight_decay: 0.0
num_train_epochs: 1
use_flash_attn: true
alpha_nll: 1.0
clip_grad_norm: -1.0
seed: 1024 #980406
update_projector_only: true
per_device_train_batch_size: 4
gradient_accumulation_steps: 8 ## assume there are 8 GPUs, so the total batch size is 384


## logging
logging_steps: 1
project_name: cformer_pretraining
exp_name: wikipedia_pretrain
# checkpointing_steps: "1000" ## string number or epoch


